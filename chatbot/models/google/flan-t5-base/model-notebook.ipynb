{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model used: google/flan-t5-base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Install dependencies and create exec file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PguvuZpg6D3N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate evaluate datasets sentencepiece rouge_score\n",
        "\n",
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import torch\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "from accelerate import Accelerator\n",
        "\n",
        "def main():\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    DATA_PATH = \"/kaggle/input/modeldata/generated_data.csv\"\n",
        "    PROMPT_COLUMN_NAME = 'prompt'\n",
        "    RESPONSE_COLUMN_NAME = 'response'\n",
        "    MODEL_CHECKPOINT = \"google/flan-t5-base\"\n",
        "    FINETUNED_MODEL_DIR = \"/kaggle/working/chatbot_finetuned_stable\"\n",
        "    TRAINING_EPOCHS = 15\n",
        "    LEARNING_RATE = 3e-5\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "        AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = split_dataset[\"train\"]\n",
        "    eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        model_inputs = tokenizer(examples[PROMPT_COLUMN_NAME], max_length=128, truncation=True)\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(examples[RESPONSE_COLUMN_NAME], max_length=128, truncation=True)\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "    tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=FINETUNED_MODEL_DIR,\n",
        "        num_train_epochs=TRAINING_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"{FINETUNED_MODEL_DIR}/logs\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        fp16=False,\n",
        "        max_grad_norm=1.0,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        trainer.save_model(FINETUNED_MODEL_DIR)\n",
        "        tokenizer.save_pretrained(FINETUNED_MODEL_DIR)\n",
        "        logging.info(f\"Best fine-tuned model saved to: {FINETUNED_MODEL_DIR}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('train.py', 'w') as f:\n",
        "    f.write(script_content)\n",
        "\n",
        "print(\"File 'train.py' created with stability fixes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHom1sv_6D3P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1IdVb4M6D3R"
      },
      "source": [
        "**General Tests**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjsNAqwu6D3T",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import evaluate\n",
        "\n",
        "def load_inference_model(model_path):\n",
        "    print(f\"Loading fine-tuned model for inference from: {model_path}\")\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"ERROR: Model directory not found at '{model_path}'.\")\n",
        "        print(\"Please run the fine-tuning script successfully before testing.\")\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "        print(f\"Model loaded successfully to device: {device}\")\n",
        "        return model, tokenizer, device\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An error occurred while loading the model: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def generate_response(prompt, model, tokenizer, device):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5, early_stopping=True)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def run_test_with_evaluation(model, tokenizer, device, test_file_path, prompt_column, response_column, num_samples=10):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting LLM Functionality and Correctness Test\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(test_file_path)\n",
        "        df.columns = df.columns.str.strip()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Test file not found at '{test_file_path}'.\")\n",
        "        return\n",
        "\n",
        "    required_columns = [prompt_column, response_column]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        print(f\"ERROR: Test file must contain '{prompt_column}' and '{response_column}' columns.\")\n",
        "        return\n",
        "\n",
        "    sample_df = df.sample(n=num_samples, random_state=42)\n",
        "    prompts = sample_df[prompt_column].tolist()\n",
        "    reference_responses = sample_df[response_column].tolist()\n",
        "\n",
        "    generated_responses = []\n",
        "    rouge_metric = evaluate.load('rouge')\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"--- Test Case {i+1}/{num_samples} ---\")\n",
        "        print(f\"PROMPT: {prompt}\")\n",
        "\n",
        "        reference = reference_responses[i]\n",
        "        print(f\"REFERENCE RESPONSE: {reference}\")\n",
        "\n",
        "        bot_response = generate_response(prompt, model, tokenizer, device)\n",
        "        generated_responses.append(bot_response)\n",
        "        print(f\"CHATBOT RESPONSE: {bot_response}\")\n",
        "\n",
        "        individual_score = rouge_metric.compute(\n",
        "            predictions=[bot_response],\n",
        "            references=[reference]\n",
        "        )\n",
        "        correctness_percentage = individual_score.get('rougeL', 0.0) * 100\n",
        "        print(f\"CORRECTNESS SCORE (ROUGE-L): {correctness_percentage:.2f}%\")\n",
        "        print(\"-\" * (len(f\"--- Test Case {i+1}/{num_samples} ---\")) + \"\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Overall Performance Evaluation\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    total_scores = rouge_metric.compute(\n",
        "        predictions=generated_responses,\n",
        "        references=reference_responses\n",
        "    )\n",
        "\n",
        "    for key, value in total_scores.items():\n",
        "        total_percentage = value * 100\n",
        "        print(f\"Total Average {key.upper()}: {total_percentage:.2f}%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    FINETUNED_MODEL_DIR = \"/kaggle/working/chatbot_finetuned_stable/checkpoint-11310\"\n",
        "    TEST_DATA_PATH = \"/kaggle/input/modeldata/generated_data.csv\"\n",
        "    PROMPT_COLUMN = 'prompt'\n",
        "    RESPONSE_COLUMN = 'response'\n",
        "\n",
        "    model, tokenizer, device = load_inference_model(FINETUNED_MODEL_DIR)\n",
        "\n",
        "    if model and tokenizer:\n",
        "        run_test_with_evaluation(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device,\n",
        "            test_file_path=TEST_DATA_PATH,\n",
        "            prompt_column=PROMPT_COLUMN,\n",
        "            response_column=RESPONSE_COLUMN,\n",
        "            num_samples=15\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFlamvul6D3V"
      },
      "source": [
        "**Identification of the best model for error cases when executing the final step of the script.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_8dr2Xn6D3W",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "output_dir = \"/kaggle/working/chatbot_finetuned_stable\"\n",
        "\n",
        "all_checkpoints = []\n",
        "if os.path.exists(output_dir):\n",
        "    all_checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
        "\n",
        "if not all_checkpoints:\n",
        "    print(f\"WARNING: No checkpoint directory found in '{output_dir}'.\")\n",
        "else:\n",
        "    latest_checkpoint = sorted(all_checkpoints, key=lambda x: int(x.split('-')[1]))[-1]\n",
        "    trainer_state_path = os.path.join(output_dir, latest_checkpoint, \"trainer_state.json\")\n",
        "\n",
        "    try:\n",
        "        with open(trainer_state_path, 'r') as f:\n",
        "            state = json.load(f)\n",
        "\n",
        "        best_model_path = state.get(\"best_model_checkpoint\")\n",
        "\n",
        "        if best_model_path:\n",
        "            print(\"=\"*50)\n",
        "            print(f\"The best model is located at: {best_model_path}\")\n",
        "            print(\"Use this path in your test/chat cell.\")\n",
        "            print(\"=\"*50)\n",
        "        else:\n",
        "            print(f\"WARNING: Unable to find the 'best_model_checkpoint' key. Using the latest checkpoint as an alternative: {os.path.join(output_dir, latest_checkpoint)}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: trainer_state.json file not found in {trainer_state_path}. Unable to determine the best model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: An error occurred while reading the state file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNpwxb1y6D3Y"
      },
      "source": [
        "**Compression and availability for download of the selected checkpoint.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSHYSmVE6D3Z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!tar -czvf checkpoint-11310.tar.gz /kaggle/working/chatbot_finetuned_stable/checkpoint-11310"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebooke3c0d1feeb",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7604805,
          "sourceId": 12084073,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
